# ğŸŒ WorldEdit (2026): Open-World Image Editing Benchmark and Model

[![Project Page](https://img.shields.io/badge/Project-Website-blue)](https://worldedit0.github.io/WorldEdit/)
[![Paper](https://img.shields.io/badge/Paper-OpenReview-red)](https://openreview.net/pdf?id=ErPfOExrrr)
[![Dataset](https://img.shields.io/badge/Dataset-HuggingFace-yellow)](https://huggingface.co/datasets/WorldEdit0/WorldEdit)
[![Model](https://img.shields.io/badge/Model-HuggingFace-green)](https://huggingface.co/WorldEdit0/WorldEdit2026)

Official implementation of **WorldEdit (2026)**.

> ğŸŒ Project Page: https://worldedit0.github.io/WorldEdit/  
> ğŸ“„ Paper: https://openreview.net/pdf?id=ErPfOExrrr  
> ğŸ¤— Benchmark & Dataset: https://huggingface.co/datasets/WorldEdit0/WorldEdit  
> ğŸ¤— Model Weights: https://huggingface.co/WorldEdit0/WorldEdit2026  

---

## ğŸ“Œ Overview

**WorldEdit** introduces:

- ğŸ”¹ A large-scale open-world image editing benchmark  
- ğŸ”¹ A standardized evaluation pipeline  
- ğŸ”¹ A strong editing model trained for open-domain editing tasks  

This repository contains:

- Model inference code  
- Benchmark construction tools  
- Evaluation scripts  
- Example notebook  


---

# ğŸš€ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/WorldEdit0/WorldEdit-2026.git
cd WorldEdit-2026
```

## 2ï¸âƒ£ Create a Conda Environment

Create a new environment:

```bash
conda create -n worldedit python=3.10 -y
conda activate worldedit
```

## 3ï¸âƒ£ Install Project Dependencies

```bash
pip install -r requirements.txt
```


# ğŸ§  Model Inference

## Step 1: Download Model Weights

Download the official WorldEdit model from Hugging Face:

```bash
huggingface-cli download WorldEdit0/WorldEdit2026 \
    --repo-type model \
    --local-dir weights/ \
    --resume-download
```

Model weights will be stored in:

```
weights/
```

---

## Step 2: Run Inference

```bash
python infer.py
```

You may also use:

```
inference.ipynb
```

for more usage.

---

# ğŸ“Š Benchmark Evaluation

WorldEdit provides a standardized benchmark for evaluation.

---

## ğŸ”¹ Step 1: Download Benchmark Dataset

```bash
cd eval

huggingface-cli download WorldEdit0/WorldEdit \
    WorldEdit/test-00000-of-00001.parquet \
    --repo-type dataset \
    --local-dir eval/worldedit_bench \
    --resume-download
```

---

## ğŸ”¹ Step 2: Build the Benchmark

```bash
cd worldedit_bench
python build_bench.py
cd ..
```

---

## ğŸ”¹ Step 3A: Evaluate Official WorldEdit Model

If you want to evaluate results generated by the official WorldEdit model:

```bash
bash infer.sh
```

---

## ğŸ”¹ Step 3B: Evaluate Your Own Model

1. Place your edited `.png` results into:

```
eval/output/
```

2. Run evaluation:

```bash
bash eval.sh
```

---

# ğŸ”‘ Evaluation API Requirement

The evaluation pipeline uses:

> **Qwen-VL-Max** on Alibaba Bailian Platform

You must obtain an API key from:

ğŸ‘‰ https://bailian.console.aliyun.com/cn-beijing/?tab=model#/api-key

Set your API key in the eval.sh before running evaluation.

---

# ğŸ“ Citation

If you find this work useful, please cite:

```bibtex
@article{worldedit2026,
  title={WorldEdit: Open-World Image Editing Benchmark and Model},
  author={Authors},
  year={2026},
  journal={OpenReview}
}
```

---

# â­ Acknowledgements

We thank the open-source community and contributors who made this project possible.

---

If you use WorldEdit in your research, please consider giving us a â­ on GitHub!
