# üåç WorldEdit: Open-World Image Editing Benchmark and Model

[![Project Page](https://img.shields.io/badge/Project-Website-blue)](https://worldedit0.github.io/WorldEdit/)
[![Paper](https://img.shields.io/badge/Paper-OpenReview-red)](https://arxiv.org/abs/2602.07095v1)
[![Dataset](https://img.shields.io/badge/Dataset-HuggingFace-yellow)](https://huggingface.co/datasets/WorldEdit0/WorldEdit)
[![Model](https://img.shields.io/badge/Model-HuggingFace-green)](https://huggingface.co/WorldEdit0/WorldEdit2026)

Official implementation of **WorldEdit**.

> üåê Project Page: https://worldedit0.github.io/WorldEdit/  
> üìÑ Paper: https://arxiv.org/abs/2602.07095v1  
> ü§ó Benchmark & Dataset: https://huggingface.co/datasets/WorldEdit0/WorldEdit  
> ü§ó Model Weights: https://huggingface.co/WorldEdit0/WorldEdit2026  

---

## üìå Overview

**WorldEdit** is a large-scale benchmark and unified framework for open-world image editing. It aims to systematically evaluate and advance image editing models beyond narrow-domain or template-based settings.

 **WorldEdit** introduces:

- üåç A large-scale open-world image editing benchmark covering diverse real-world scenarios  
- üìä A standardized and reproducible evaluation pipeline  
- üß† A strong editing model designed for general-purpose open-domain editing  
 

To ensure fair and scalable evaluation, we provide an automated assessment framework built upon vision-language models, enabling consistent scoring across different editing systems.

This repository includes:

- üîß Model inference code for WorldEdit  
- üóÇ Benchmark construction tools  
- üìà Evaluation scripts and scoring pipeline  

For more details, please refer to the [project page](https://worldedit0.github.io/WorldEdit/) and the [paper](https://openreview.net/pdf?id=ErPfOExrrr).


## üöÄ Installation

### 1Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/WorldEdit0/WorldEdit-2026.git
cd WorldEdit-2026
```

### 2Ô∏è‚É£ Create a Conda Environment

Create a new environment:

```bash
conda create -n worldedit python=3.10 -y
conda activate worldedit
```

### 3Ô∏è‚É£ Install Project Dependencies

```bash
pip install -r requirements.txt
```


## üß† Model Inference

### Step 1: Download Model Weights

Download the official WorldEdit model from Hugging Face:

```bash
huggingface-cli download WorldEdit0/WorldEdit2026 \
    --repo-type model \
    --local-dir weights/ \
    --resume-download
```
---

### Step 2: Run Inference

```bash
python infer.py
```

---

## üìä Benchmark Evaluation

WorldEdit provides a standardized benchmark for evaluation.

---

### üîπ Step 1: Download Benchmark Dataset

```bash
cd eval
huggingface-cli download WorldEdit0/WorldEdit \
    WorldEdit/test-00000-of-00001.parquet \
    --repo-type dataset \
    --local-dir worldedit_bench \
    --resume-download
```

---

### üîπ Step 2: Build the Benchmark

```bash
cd worldedit_bench
python build_bench.py
cd ..
```

---

### üîπ Step 3A: Evaluate Official WorldEdit Model

If you want to evaluate results generated by the official WorldEdit model:

```bash
bash infer.sh
```

---

### üîπ Step 3B: Evaluate Your Own Model

1. Place your edited `.png` results into:

```
eval/output/
```

2. Run evaluation:

```bash
bash eval.sh
```

---

## üîë Evaluation API Requirement

The evaluation pipeline uses:

> **Qwen-VL-Max** on Alibaba Bailian Platform

You must obtain an API key from:

üëâ https://bailian.console.aliyun.com/cn-beijing/?tab=model#/api-key

Set your API key in the eval.sh before running evaluation.

---

## üìù Citation

If you find this work useful, please cite:

```bibtex
@misc{lin2026worldeditopenworldimageediting,
      title={WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark}, 
      author={Wang Lin and Feng Wang and Majun Zhang and Wentao Hu and Tao Jin and Zhou Zhao and Fei Wu and Jingyuan Chen and Alan Yuille and Sucheng Ren},
      year={2026},
      eprint={2602.07095},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2602.07095}, 
}
```

---

## ‚≠ê Acknowledgements

We thank the open-source community and contributors who made this project possible.

---

If you use WorldEdit in your research, please consider giving us a ‚≠ê on GitHub!
