# ğŸŒ WorldEdit: Open-World Image Editing Benchmark and Model

[![Project Page](https://img.shields.io/badge/Project-Website-blue)](https://worldedit0.github.io/WorldEdit/)
[![Paper](https://img.shields.io/badge/Paper-OpenReview-red)](https://openreview.net/pdf?id=ErPfOExrrr)
[![Dataset](https://img.shields.io/badge/Dataset-HuggingFace-yellow)](https://huggingface.co/datasets/WorldEdit0/WorldEdit)
[![Model](https://img.shields.io/badge/Model-HuggingFace-green)](https://huggingface.co/WorldEdit0/WorldEdit2026)

Official implementation of **WorldEdit**.

> ğŸŒ Project Page: https://worldedit0.github.io/WorldEdit/  
> ğŸ“„ Paper: https://openreview.net/pdf?id=ErPfOExrrr  
> ğŸ¤— Benchmark & Dataset: https://huggingface.co/datasets/WorldEdit0/WorldEdit  
> ğŸ¤— Model Weights: https://huggingface.co/WorldEdit0/WorldEdit2026  

---

## ğŸ“Œ Overview

**WorldEdit** is a large-scale benchmark and unified framework for open-world image editing. It aims to systematically evaluate and advance image editing models beyond narrow-domain or template-based settings.

 **WorldEdit** introduces:

- ğŸŒ A large-scale open-world image editing benchmark covering diverse real-world scenarios  
- ğŸ“Š A standardized and reproducible evaluation pipeline  
- ğŸ§  A strong editing model designed for general-purpose open-domain editing  
 

To ensure fair and scalable evaluation, we provide an automated assessment framework built upon vision-language models, enabling consistent scoring across different editing systems.

This repository includes:

- ğŸ”§ Model inference code for WorldEdit  
- ğŸ—‚ Benchmark construction tools  
- ğŸ“ˆ Evaluation scripts and scoring pipeline  

For more details, please refer to the [project page](https://worldedit0.github.io/WorldEdit/) and the [paper](https://openreview.net/pdf?id=ErPfOExrrr).


## ğŸš€ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/WorldEdit0/WorldEdit-2026.git
cd WorldEdit-2026
```

### 2ï¸âƒ£ Create a Conda Environment

Create a new environment:

```bash
conda create -n worldedit python=3.10 -y
conda activate worldedit
```

### 3ï¸âƒ£ Install Project Dependencies

```bash
pip install -r requirements.txt
```


## ğŸ§  Model Inference

### Step 1: Download Model Weights

Download the official WorldEdit model from Hugging Face:

```bash
huggingface-cli download WorldEdit0/WorldEdit2026 \
    --repo-type model \
    --local-dir weights/ \
    --resume-download
```
---

### Step 2: Run Inference

```bash
python infer.py
```

---

## ğŸ“Š Benchmark Evaluation

WorldEdit provides a standardized benchmark for evaluation.

---

### ğŸ”¹ Step 1: Download Benchmark Dataset

```bash
cd eval

huggingface-cli download WorldEdit0/WorldEdit \
    WorldEdit/test-00000-of-00001.parquet \
    --repo-type dataset \
    --local-dir eval/worldedit_bench \
    --resume-download
```

---

### ğŸ”¹ Step 2: Build the Benchmark

```bash
cd worldedit_bench
python build_bench.py
cd ..
```

---

### ğŸ”¹ Step 3A: Evaluate Official WorldEdit Model

If you want to evaluate results generated by the official WorldEdit model:

```bash
bash infer.sh
```

---

### ğŸ”¹ Step 3B: Evaluate Your Own Model

1. Place your edited `.png` results into:

```
eval/output/
```

2. Run evaluation:

```bash
bash eval.sh
```

---

## ğŸ”‘ Evaluation API Requirement

The evaluation pipeline uses:

> **Qwen-VL-Max** on Alibaba Bailian Platform

You must obtain an API key from:

ğŸ‘‰ https://bailian.console.aliyun.com/cn-beijing/?tab=model#/api-key

Set your API key in the eval.sh before running evaluation.

---

## ğŸ“ Citation

If you find this work useful, please cite:

```bibtex
@article{worldedit2026,
  title={WorldEdit: Open-World Image Editing Benchmark and Model},
  author={Authors},
  year={2026},
  journal={OpenReview}
}
```

---

## â­ Acknowledgements

We thank the open-source community and contributors who made this project possible.

---

If you use WorldEdit in your research, please consider giving us a â­ on GitHub!
